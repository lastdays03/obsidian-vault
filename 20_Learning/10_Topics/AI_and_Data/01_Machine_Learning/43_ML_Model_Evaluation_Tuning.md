---
Source: study_machine_learning
---

# λ¨λΈ ν‰κ°€μ™€ ν•μ΄νΌνλΌλ―Έν„° νλ‹

## 1. κ°μ” λ° λ©ν‘
1.  **κ³Όμ ν•©(Overfitting)**μ μ„ν—μ„±μ„ μ§μ ‘ ν™•μΈν•©λ‹λ‹¤.
2.  μ΄λ¥Ό λ°©μ§€ν•κΈ° μ„ν• **κµμ°¨ κ²€μ¦(Cross Validation)** κΈ°λ²•λ“¤(K-Fold, Stratified K-Fold)μ„ μ‹¤μµν•©λ‹λ‹¤.
3.  μµμ μ μ„±λ¥μ„ λ‚΄λ” ν•μ΄νΌνλΌλ―Έν„°λ¥Ό μ°ΎκΈ° μ„ν•΄ **GridSearchCV**λ¥Ό μ‚¬μ©ν•©λ‹λ‹¤.

## 1. κ³Όμ ν•©(Overfitting)μ ν•¨μ •
**Why**: ν•™μµ λ°μ΄ν„°λ΅ κ·Έλ€λ΅ μ‹ν—μ„ μΉλ©΄ 100μ μ΄ λ‚μµλ‹λ‹¤. ν•μ§€λ§ μ΄κ±΄ 'μ•”κΈ°'μ΄μ§€ 'ν•™μµ'μ΄ μ•„λ‹™λ‹λ‹¤. μƒλ΅μ΄ λ°μ΄ν„°μ—μ„λ” ν‹€λ¦΄ μ μμµλ‹λ‹¤.

```python
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# ν•™μµ λ°μ΄ν„°λ΅λ§ ν•™μµν•κ³  μμΈ΅ν•λ©΄?
dt_clf = DecisionTreeClassifier(random_state=123)
dt_clf.fit(X, y)
pred = dt_clf.predict(X)

print(f"ν•™μµ λ°μ΄ν„°λ΅ ν‰κ°€ν• μ •ν™•λ„: {accuracy_score(y, pred):.4f} (μ™„λ²½ν•΄λ³΄μ΄μ§€λ§ μ„ν—ν•¨)")
```
> **Output**: ν•™μµ λ°μ΄ν„°λ΅ ν‰κ°€ν• μ •ν™•λ„: 1.0000 (μ™„λ²½ν•΄λ³΄μ΄μ§€λ§ μ„ν—ν•¨)

## 2. ν•™μµ/ν…μ¤νΈ λ°μ΄ν„° λ¶„λ¦¬
κ°€μ¥ κΈ°λ³Έμ μΈ μΌλ°ν™” μ„±λ¥ κ²€μ¦ λ°©λ²•μ…λ‹λ‹¤.

```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=121)

dt_clf.fit(X_train, y_train)
pred = dt_clf.predict(X_test)
print(f"ν…μ¤νΈ λ°μ΄ν„°λ΅ ν‰κ°€ν• μ •ν™•λ„: {accuracy_score(y_test, pred):.4f}")
```
> **Output**: ν…μ¤νΈ λ°μ΄ν„°λ΅ ν‰κ°€ν• μ •ν™•λ„: 0.9556

## 3. κµμ°¨ κ²€μ¦ (K-Fold Cross Validation)
**Why**: λ°μ΄ν„°κ°€ μ μ„ λ•, ν…μ¤νΈ λ°μ΄ν„°κ°€ μ°μ—°ν μ‰¬μ΄ λ¬Έμ λ“¤λ§ λ½‘ν”μ„ μλ„ μμµλ‹λ‹¤. λ°μ΄ν„°λ¥Ό Kκ°λ΅ μΌκ°μ„ λ²κ°μ•„κ°€λ©° λ‹¤ ν…μ¤νΈν•΄λ³΄λ” λ°©λ²•μ…λ‹λ‹¤. (λ°μ΄ν„°λ¥Ό μ•λ°ν•κ² μ‚¬μ©)

```python
from sklearn.model_selection import KFold
import numpy as np

kfold = KFold(n_splits=5)
cv_accuracy = []

print("--- K-Fold μ‹μ‘ ---")
for i, (train_idx, test_idx) in enumerate(kfold.split(X)):
    X_train_cv, X_test_cv = X[train_idx], X[test_idx]
    y_train_cv, y_test_cv = y[train_idx], y[test_idx]
    
    dt_clf.fit(X_train_cv, y_train_cv)
    pred_cv = dt_clf.predict(X_test_cv)
    acc = accuracy_score(y_test_cv, pred_cv)
    
    cv_accuracy.append(acc)
    print(f"#{i+1} κ²€μ¦ μ •ν™•λ„: {acc:.4f}")

print(f"\nν‰κ·  μ •ν™•λ„: {np.mean(cv_accuracy):.4f}")
```
> **Output**: ν‰κ·  μ •ν™•λ„: 0.9200

## 4. Stratified K-Fold
**Why**: λ¶“κ½ƒ λ°μ΄ν„°μ²λΌ λ μ΄λΈ”μ΄ λ¶κ· ν•ν•κ±°λ‚ λ¶„λ¥ λ¬Έμ μΌ λ•λ” K-Foldκ°€ μ„ν—ν•  μ μμµλ‹λ‹¤. (νΉμ • ν’μΆ…μ΄ ν•™μµμ…‹μ— μ•„μ μ—†μ„ μλ„ μμ). Stratified K-Foldλ” μ •λ‹µ λΉ„μ¨μ„ μ μ§€ν•λ©΄μ„ λ‚λ•λ‹λ‹¤.

```python
from sklearn.model_selection import StratifiedKFold

skf = StratifiedKFold(n_splits=3)
cv_accuracy = []

print("--- Stratified K-Fold μ‹μ‘ ---")
# splitν•  λ• y(λ μ΄λΈ”)λ¥Ό λ„£μ–΄μ¤μ•Ό λΉ„μ¨μ„ λ§μ¶ μ μμ
for i, (train_idx, test_idx) in enumerate(skf.split(X, y)):
    X_train_cv, X_test_cv = X[train_idx], X[test_idx]
    y_train_cv, y_test_cv = y[train_idx], y[test_idx]
    
    dt_clf.fit(X_train_cv, y_train_cv)
    acc = accuracy_score(y_test_cv, dt_clf.predict(X_test_cv))
    
    cv_accuracy.append(acc)
    print(f"#{i+1} κ²€μ¦ μ •ν™•λ„: {acc:.4f}")

print(f"\nStratified ν‰κ·  μ •ν™•λ„: {np.mean(cv_accuracy):.4f}")
```
> **Output**: Stratified ν‰κ·  μ •ν™•λ„: 0.9667

## 5. GridSearchCV (ν•μ΄νΌνλΌλ―Έν„° νλ‹ + κµμ°¨κ²€μ¦)
**Why**: λ¨λΈμ μ„±λ¥μ„ λ†’μ΄λ ¤λ©΄ νλΌλ―Έν„°(νΈλ¦¬ κΉμ΄ λ“±)λ¥Ό μ΅°μ ν•΄μ•Ό ν•©λ‹λ‹¤. μΌμΌμ΄ ν•΄λ³΄κΈ° νλ“λ‹, λ²”μ„λ¥Ό μ§€μ •ν•΄μ£Όλ©΄ μ•μ•„μ„ μµμ μ μ΅°ν•©μ„ μ°Ύμ•„μ¤λ‹λ‹¤.

```python
from sklearn.model_selection import GridSearchCV

# ν…μ¤νΈν•  νλΌλ―Έν„° μ΅°ν•©
params = {
    'max_depth': [1, 2, 3],
    'min_samples_split': [2, 3]
}

# GridSearchCV κ°μ²΄ μƒμ„± (refit=True: μµμ  νλΌλ―Έν„°λ΅ λ‹¤μ‹ ν•™μµμ‹μΌλ‘ )
grid_dtree = GridSearchCV(dt_clf, param_grid=params, cv=3, refit=True)

# νλ‹ μν–‰
grid_dtree.fit(X_train, y_train)

print(f"μµμ  νλΌλ―Έν„°: {grid_dtree.best_params_}")
print(f"μµκ³  μ •ν™•λ„: {grid_dtree.best_score_:.4f}")

# μµμ  λ¨λΈλ΅ μµμΆ… ν…μ¤νΈ
best_model = grid_dtree.best_estimator_
final_pred = best_model.predict(X_test)
print(f"μµμΆ… ν…μ¤νΈ μ •ν™•λ„: {accuracy_score(y_test, final_pred):.4f}")
```
> **Output**:
> μµμ  νλΌλ―Έν„°: {'max_depth': 3, 'min_samples_split': 2}
> μµκ³  μ •ν™•λ„: 0.9429
> μµμΆ… ν…μ¤νΈ μ •ν™•λ„: 0.9556

## 6. μΈμ‚¬μ΄νΈ λ„μ¶ (Insights)

### π Key Takeaways
*   **κ³Όμ ν•©(Overfitting)μ μ„ν—**: ν•™μµ λ°μ΄ν„°λ΅ ν‰κ°€ν–μ„ λ•μ 100% μ •ν™•λ„λ” 'μ‹¤λ ¥'μ΄ μ•„λ‹λΌ 'μ•”κΈ°'μΌ μ μμμ„ ν™•μΈν–μµλ‹λ‹¤.
*   **κµμ°¨ κ²€μ¦(Cross Validation)**: λ°μ΄ν„°λ¥Ό μ—¬λ¬ λ² μΌκ°μ„ κ²€μ¦ν•¨μΌλ΅μ¨, μ°μ—°μ— μν• μ„±λ¥ λ»¥ν€κΈ°λ¥Ό λ°©μ§€ν•κ³  λ¨λΈμ **μΌλ°ν™” μ„±λ¥**μ„ μ‹ λΆ°ν•  μ μκ² λμ—μµλ‹λ‹¤.
*   **Stratified K-Fold**: λ¶„λ¥ λ¬Έμ , νΉν μ •λ‹µ λΉ„μ¨μ΄ λ¶κ· ν•ν•  λ•λ” λ°λ“μ‹ μ •λ‹µ λΉ„μ¨μ„ μ μ§€ν•λ©° μΌκ°λ” **Stratified** λ°©μ‹μ΄ ν•„μ”ν•©λ‹λ‹¤.
*   **GridSearchCV**: μµμ μ ν•μ΄νΌνλΌλ―Έν„°λ¥Ό μ°Ύλ” κ³Όμ •μ„ μλ™ν™”ν•μ—¬, μλ™ λ…Έκ°€λ‹¤ μ—†μ΄ μ„±λ¥μ„ 1%λΌλ„ λ” λμ–΄μ¬λ¦΄ μ μμ—μµλ‹λ‹¤.

### π”¬ Try More
*   **νλΌλ―Έν„° λ²”μ„ ν™•μ¥**: `max_depth`λ¥Ό `[1, 2, 3]`μ΄ μ•„λ‹λΌ `[1, 3, 5, 10, None]` λ“±μΌλ΅ λ„“ν€μ„ μ‹¤ν—ν•΄λ³΄μ„Έμ”.
*   **scoring λ³€κ²½**: `accuracy` λ€μ‹  `f1`μ΄λ‚ `roc_auc` μ μλ¥Ό κΈ°μ¤€μΌλ΅ μµμ  λ¨λΈμ„ μ°Ύμ•„λ³΄μ„Έμ”. (λ‹¨, μ΄μ§„ λ¶„λ¥μΈ κ²½μ° μ£Όμ ν•„μ”)
