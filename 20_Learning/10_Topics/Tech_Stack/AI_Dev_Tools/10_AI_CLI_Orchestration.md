---
tags:
  - knowledge/topic
  - tool/cli
  - tool/ai-orchestration
Source: AI CLI 오케스트레이션(Orchestration)
---

# 10. AI CLI 오케스트레이션 (Orchestration)

클로드(Claude), 제미나이(Gemini), 코덱스(Codex/GitHub Copilot) 등 각기 다른 강점을 가진 AI 모델들을 CLI 환경에서 유기적으로 연결하여 사용하는 **'AI CLI 오케스트레이션(Orchestration)'**에 대해 분석해 드립니다.

이는 단일 모델의 한계를 극복하고, **"적재적소(Best-of-Breed)"** 전략을 통해 터미널 작업의 효율을 극대화하는 최신 트렌드입니다.

---

### 1. 왜 오케스트레이션인가? (각 모델의 역할 분담)

모든 모델이 만능은 아닙니다. 오케스트레이션의 핵심은 각 모델의 **'특기'**를 살려 파이프라인을 구성하는 것입니다.

| **모델/도구**                | **별명**          | **핵심 강점 (Specialty)**                                                  | **추천 역할**                                                   |
| :--------------------------- | :---------------- | :------------------------------------------------------------------------- | :-------------------------------------------------------------- |
| **Claude Sonnet/Opus 4.5**   | **The Architect** | 최고 코딩 성능 (SWE-bench 77~80%+), 복잡한 리팩토링, 긴 호흡 에이전트 작업 | 메인 로직/아키텍처 설계, 버그 분석, 대규모 코드베이스 작업      |
| **Gemini 3 Pro**             | **The Librarian** | 1M+ 토큰 컨텍스트, 멀티모달(코드+이미지+비디오), 대용량 데이터 분석        | 전체 코드베이스 분석, 대용량 로그/문서 해석, 멀티모달 작업      |
| **Gemini CLI / Claude Code** | **The Operator**  | 터미널 네이티브 에이전트, 빠른 쉘/파일 조작, 실시간 명령어 실행            | 터미널 명령어 추천, 단순 스크립트/에이전트 작업, 빠른 반복 개발 |

---

### 1-1. 2026년 트렌드: Multi-Model + Agentic Orchestration

단순히 하나의 LLM을 쓰는 시대는 지났습니다. 2026년 현재 개발자들은 **Multi-Model 접근**을 선호합니다.

*   **The Architect (Claude 4.5)**: SWE-bench Verified 77~80%+ 점수로 실제 소프트웨어 엔지니어링 작업 독점.
*   **The Librarian (Gemini 3 Pro)**: 2M 토큰 컨텍스트로 대용량 로그/코드베이스 처리의 표준.
*   **Agentic Tools**: `Claude Code`, `Gemini CLI` 등이 쉘 명령 실행, 파일 편집을 자율적으로 수행하는 "Agentic CLI"가 대세가 되었습니다.
*   **Alternatives**: 실시간성이 중요한 경우 **Grok 4**, 비용 효율을 위해 **DeepSeek V3**를 섞어 쓰는 것도 좋은 전략입니다.

---

### 2. CLI 오케스트레이션 구현 패턴

이 모델들을 따로 쓰는 것이 아니라, 하나의 워크플로우로 묶는 3가지 방법입니다. 각 패턴이 기술적으로 어떻게 동작하고 장단점이 무엇인지 심층 분석합니다.

#### ① 유니파이드 인터페이스 (Unified Interface)

**"하나의 그릇(Tool)에 여러 두뇌(Model)를 갈아 끼우는 전략"**

가장 직관적이며, **대화의 맥락(Context)을 유지**하면서 모델의 지능 수준을 조절할 수 있다는 것이 핵심입니다.

* **기술적 메커니즘 (How it works):**
    * **추상화 계층 (Abstraction Layer):** `Aider`나 `Cline` 같은 도구는 OpenAI, Anthropic, Google 등의 서로 다른 API 규격(Request/Response format)을 하나의 표준 포맷으로 변환하여 처리합니다.
    * **컨텍스트 관리 (State Management):** 사용자가 모델을 바꿔도(`--model` 변경), 지금까지의 대화 내역(Chat History)과 분석 중인 파일 정보(File Context)는 로컬 메모리에 그대로 유지됩니다.
    * **토큰 윈도우 조정:** Gemini(2M 토큰)에서 Claude(200k 토큰)로 전환할 때, 도구는 자동으로 컨텍스트 크기를 조절하거나 오래된 대화를 가지치기(Pruning)하여 오류를 방지합니다.

* **심층 분석 (Pros & Cons):**
    * **장점:** 인지 부하 감소(명령어 통일), 작업 연속성 보장. (예: Gemini로 넓게 훑어보고, 그 상태 그대로 Claude로 깊게 수정)
    * **단점:** "최소 공통분모" 문제. 특정 모델만 가진 고유 기능(예: Gemini의 비디오 입력, OpenAI의 특정 함수 호출)을 완벽하게 활용하지 못할 수 있습니다.

* **최적 활용 전략:**
    * **비용 최적화:** 코딩 초기 단계나 단순 질의는 저렴한 모델(Haiku, Flash)로 진행하여 컨텍스트를 쌓고, 해결이 안 되는 난관에 봉착했을 때만 비싼 모델(Sonnet, Opus)로 스위칭합니다.

#### ② 파이프라인 체이닝 (Pipeline Chaining)

**"데이터의 흐름을 제어하여 공정(Process)을 자동화하는 전략"**

유닉스의 파이프(`|`) 철학을 따르며, 앞 단계의 출력이 뒷 단계의 입력이 되도록 **단방향 워크플로우**를 설계하는 것입니다.

* **기술적 메커니즘 (How it works):**
    * **Stateless (상태 비저장):** 각 단계(Step)는 독립적입니다. 앞 단계의 모델이 무슨 생각을 했는지 뒷 모델은 모릅니다. 오직 전달받은 텍스트(stdin)만 보고 판단합니다.
    * **프롬프트 글루(Glue):** 각 파이프 사이에는 앞 모델의 출력을 뒷 모델이 이해하기 좋게 다듬는 '프롬프트 엔지니어링'이 접착제 역할을 합니다. (주로 JSON 포맷이 데이터 교환용으로 많이 쓰입니다.)

* **심층 분석 (Pros & Cons):**
    * **장점:**
        * **재현성(Reproducibility):** 스크립트로 저장해두면 언제든 똑같은 프로세스를 반복 실행할 수 있습니다. (일일 보고서 생성, 로그 분석 등)
        * **속도(Parallelism):** `xargs` 등을 이용해 앞 단계를 병렬로 처리하고, 결과를 취합하는 맵-리듀스(Map-Reduce) 구현이 가장 쉽습니다.
    * **단점:** **단절성.** 중간 단계에서 데이터가 오염되면(예: 이상한 포맷 출력) 그 다음 단계가 연쇄적으로 실패합니다. 에러 핸들링이 까다롭습니다.

* **최적 활용 전략:**
    * **정제 후 가공:** "Raw Data(대량)" → `Cheap Model` (필터링/요약) → "Refined Data(소량)" → `Smart Model` (최종 산출물 작성) 구조로 설계하여 토큰 비용을 획기적으로 줄입니다.

#### ③ 에이전트 위임 (Agent Delegation)

**"메인 두뇌(Controller)가 필요에 따라 도구(Sub-Agent)를 호출하는 전략"**

가장 진보된 형태로, 사용자가 흐름을 지정하는 것이 아니라 **AI가 스스로 판단하여** 다른 모델을 '도구'처럼 사용합니다.

* **기술적 메커니즘 (How it works):**
    * **MCP & Function Calling:** 메인 모델(주로 Claude 3.5 Sonnet)에게 "너는 `summarize_with_gemini`라는 함수를 쓸 수 있어"라고 알려줍니다.
    * **자율적 라우팅:** 사용자가 "이거 분석해줘"라고만 해도, Claude가 "내용이 너무 기네? 내가 읽기엔 비효율적이야. Gemini에게 요약 요청을 보내자"라고 판단(Reasoning)하고 API를 쏘고 결과를 받아옵니다.
    * **OS와 App의 관계:** 메인 에이전트는 운영체제(OS) 역할을, 서브 에이전트(Gemini, Local LLM)는 특정 기능을 수행하는 애플리케이션 역할을 합니다.

* **심층 분석 (Pros & Cons):**
    * **장점:** **완전한 자율성.** 사용자가 어떤 모델을 쓸지 고민할 필요가 없습니다. 복잡하고 동적인 문제 해결(예: "웹 검색해서 최신 정보 찾고, 로컬 DB랑 비교해줘")에 최적화되어 있습니다.
    * **단점:**
        * **복잡성:** 초기 구축 난이도가 높습니다(MCP 서버 설정 등).
        * **지연 시간(Latency):** 모델이 다른 모델을 호출하고 결과를 기다리는 과정에서 응답 속도가 느려질 수 있습니다.

* **최적 활용 전략:**
    * **Cursor/Cline + MCP:** 현재 개발자님 환경에서는, 메인 에이디터(Cursor)에 MCP 서버를 연동하여, 로컬 파일 탐색이나 웹 검색 같은 '특수 기능'을 외부 에이전트에게 위임하는 방식이 곧 표준이 될 것입니다.

#### 📊 3가지 패턴 비교 요약

| 특징          | 1. Unified Interface    | 2. Pipeline Chaining            | 3. Agent Delegation             |
| ------------- | ----------------------- | ------------------------------- | ------------------------------- |
| **핵심 컨셉** | **스위칭 (Switching)**  | **흐름 (Flow)**                 | **관리 (Managing)**             |
| **주도권**    | 사용자 (직접 모델 변경) | 스크립트 (사전 정의된 순서)     | **AI (메인 모델의 판단)**       |
| **연결성**    | 대화 맥락(Context) 공유 | 입/출력(IO)만 공유              | 함수 호출(RPC)로 연결           |
| **추천 상황** | 코딩, 인터랙티브 디버깅 | 로그 분석, 문서 정리, 배치 작업 | 복합적 문제 해결, 자율 에이전트 |
| **대표 도구** | Aider, Cline, Cursor    | llm (CLI), Shell Script         | MCP, LangChain                  |
    

---

### 3. 실전 오케스트레이션 워크플로우 제안

개발자님의 환경(맥, 파이썬, 데이터 분석)에 맞춘 최적의 조합입니다.

**상황: 데이터 분석 프로젝트 진행**

1. **환경 세팅 (Copilot CLI):**
    
    - 터미널 명령어가 기억 안 날 때 사용합니다.
        
    - `gh copilot suggest "가상환경 만들고 판다스 설치하는 명령어 알려줘"` → 바로 실행.
        
2. **대규모 데이터/로그 분석 (Gemini CLI):**
    
    - 데이터 샘플이 너무 크거나, 참고할 논문 PDF가 있을 때 사용합니다.
        
    - `llm -m gemini-3.0-pro -a paper.pdf "이 논문의 수식을 파이썬 코드로 변환해줘"`
        
3. **코드 작성 및 리팩토링 (Claude via Cursor/Aider):**
    
    - Gemini가 뽑아준 초안을 바탕으로 실제 프로덕션 코드를 짭니다.
        
    - "이 코드를 안티그래비티 워크플로우에 맞춰서 클래스 구조로 바꿔줘."
        

---

### 4. 핵심 도구 추천 (Tool Stack)

이 오케스트레이션을 가능하게 하는 구체적인 CLI 도구들입니다.

1. **[LLM CLI](https://llm.datasette.io/) (by Simon Willison):**
    
    - **필수 설치.** 파이프라인 체이닝의 핵심입니다. 플러그인을 통해 Claude, Gemini, GPT, 로컬 모델(Llama)을 모두 CLI에서 다룰 수 있습니다.
        
2. **[Claude Code](https://docs.anthropic.com/claude/docs/claude-code) / [Gemini CLI](https://ai.google.dev/gemini-api/docs/cli):**
    
    - **The Operator.** 터미널 네이티브 에이전트로, 단순 질의를 넘어 파일 편집과 쉘 실행을 직접 수행합니다. 2026년 이후 `gh copilot`을 대체하는 추세입니다.
        
3. **[Aider](https://aider.chat/):**
    
    - **The Pair Programmer.** Git 저장소 전체를 이해하며, 작업 도중 모델 변경( `/model ...`)이 매우 자유롭습니다. Claude 4.5와의 조합이 가장 강력합니다.
        

---

### 💡 요약 및 제언

"오케스트레이션"이라고 해서 거창한 시스템을 구축할 필요는 없습니다. 가장 먼저 **`llm` CLI** 도구를 설치해서 터미널에서 **Gemini로 읽고 Claude로 쓰는** 경험을 해보시는 것을 추천합니다.

👉 **실전 가이드: [[11_AI_Team_Operation_Guide|AI 팀 운영 가이드 (AI Team Operation Guide)]]** - AI 직원(페르소나) 정의 및 팀 운영 방법


**다음 단계로, `llm` CLI를 설치하고 API 키를 등록하여 파이프라인을 구축하는 명령어를 정리해 드릴까요?**