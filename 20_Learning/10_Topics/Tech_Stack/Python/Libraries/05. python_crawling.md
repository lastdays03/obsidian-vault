---
tags:
  - knowledge/topic
Source: [[GitHub_Study_Python]]
---

# 05. Python 웹 크롤링 (Web Crawling)

## 📖 정의 (Definition)
**웹 크롤링(Web Crawling)**은 웹 페이지에서 필요한 데이터를 자동으로 수집하는 기술입니다. 정적 페이지를 위한 Requests/BeautifulSoup와 동적 페이지를 위한 Selenium/Playwright 등을 활용합니다.

---

이 문서는 Python을 활용한 웹 크롤링 및 스크래핑 기술을 다룹니다. 정적 페이지 수집부터 동적 페이지 제어, 그리고 대규모 수집을 위한 프레임워크까지 포괄합니다.

---

## 목차 (Table of Contents)

1.  [웹 크롤링 개요](#1-웹-크롤링-개요)
2.  [정적 크롤링: Requests + BeautifulSoup4](#2-정적-크롤링-requests--beautifulsoup4)
    *   [2.1 Requests](#21-requests)
    *   [2.2 BeautifulSoup4](#22-beautifulsoup4)
    *   [2.3 정적 크롤링 문법 요약](#23-정적-크롤링-문법-요약)
3.  [동적 크롤링: 브라우저 자동화](#3-동적-크롤링-브라우저-자동화)
    *   [3.1 Selenium](#31-selenium)
    *   [3.2 Playwright](#32-playwright)
    *   [3.3 동적 크롤링 문법 요약](#33-동적-크롤링-문법-요약)
4.  [크롤링 프레임워크: Scrapy](#4-크롤링-프레임워크-scrapy)
5.  [심화 주제](#5-심화-주제)
6.  [도구 비교 및 요약](#6-도구-비교-및-요약)

---

## 0. 선수 지식 (Prerequisites)

이 문서를 학습하기 전에 다음 내용을 먼저 확인하면 좋습니다:
*   **[15. python_popular_libraries.md](15.%20python_popular_libraries.md)**: **Requests**와 **BeautifulSoup4**의 기본적인 사용법이 소개되어 있습니다.

---

## 1. 웹 크롤링 개요

### 1.1 크롤링(Crawling) vs 스크래핑(Scraping)
*   **크롤링**: 웹 페이지의 링크를 따라가며 웹사이트의 구조를 탐색하고 인덱싱하는 과정 (예: 구글 검색봇).
*   **스크래핑**: 특정 웹 페이지에서 원하는 데이터를 추출하는 과정. 실무에서는 보통 이 둘을 혼용해서 사용합니다.

### 1.2 주의사항 및 윤리
*   **robots.txt 확인**: `https://example.com/robots.txt`를 확인하여 수집 허용 범위를 준수해야 합니다.
*   **서버 부하 방지**: `time.sleep()` 등을 사용하여 요청 간격을 두어야 합니다.
*   **User-Agent 설정**: 봇이 아닌 일반 브라우저처럼 보이게 하여 차단을 방지합니다.

---

## 2. 정적 크롤링: Requests + BeautifulSoup4

서버로부터 HTML 원본을 받아와서 파싱하는 방식입니다. JavaScript로 렌더링되지 않는 정적인 페이지에 적합하며 속도가 빠릅니다.

### 2.1 Requests
HTTP 요청을 보내는 라이브러리입니다.
```python
import requests

url = 'https://www.example.com'
headers = {'User-Agent': 'Mozilla/5.0 ...'} # 브라우저 정보 위장
response = requests.get(url, headers=headers)

if response.status_code == 200:
    html = response.text
```

### 2.2 BeautifulSoup4
HTML 코드를 파싱하여 원하는 태그를 쉽게 찾을 수 있게 해줍니다.
```python
from bs4 import BeautifulSoup

soup = BeautifulSoup(html, 'html.parser')

# 태그 찾기
title = soup.find('h1').text
items = soup.select('ul.list > li') # CSS Selector 사용

for item in items:
    print(item.text)
### 2.3 정적 크롤링 문법 요약

Requests와 BeautifulSoup4의 핵심 문법입니다.

| 라이브러리 | 함수/메서드 | 용도 | 예제 |
| :--- | :--- | :--- | :--- |
| **Requests** | `requests.get(url)` | GET 요청 보내기 | `res = requests.get('http://...')` |
| | `res.status_code` | 응답 상태 확인 | `if res.status_code == 200:` |
| | `res.text` | 응답 본문(HTML) | `html = res.text` |
| **BS4** | `BeautifulSoup(html, 'html.parser')` | 파서 초기화 | `soup = BeautifulSoup(html, ...)` |
| | `soup.find('tag')` | 첫 번째 태그 찾기 | `soup.find('h1')` |
| | `soup.find_all('tag')` | 모든 태그 찾기 (리스트) | `soup.find_all('a')` |
| | `soup.select('css_selector')` | CSS 선택자로 찾기 | `soup.select('div.content > p')` |
| | `tag.text` | 태그 내 텍스트 추출 | `item.text` |
| | `tag['href']` | 속성 값 추출 | `link['href']` |

```python
# 실전 패턴
import requests
from bs4 import BeautifulSoup

res = requests.get('https://news.example.com')
soup = BeautifulSoup(res.text, 'html.parser')

# 뉴스 제목 가져오기 (CSS Selector 활용)
titles = soup.select('.news-list > li > a.title')
for title in titles:
    print(title.text.strip()) # 공백 제거
    print(title['href'])      # 링크 주소
```

> **Tip**: 더 복잡한 텍스트 패턴(이메일, 전화번호 등)을 추출해야 한다면 **[[12. python_regular_expressions|정규표현식]]**를 참고하여 정규표현식을 활용하세요.

---

## 3. 동적 크롤링: 브라우저 자동화

JavaScript로 내용이 동적으로 생성되거나, 로그인/클릭 등 상호작용이 필요한 경우 사용합니다. 실제 브라우저를 띄우거나 헤드리스(Headless) 모드로 실행합니다.

### 3.1 Selenium
가장 널리 알려진 브라우저 자동화 도구입니다. 안정적이지만 속도가 상대적으로 느립니다.

```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager

# 브라우저 설정
options = webdriver.ChromeOptions()
# options.add_argument('headless') # 창 없이 실행

driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

driver.get('https://www.example.com')

# 요소 찾기 및 상호작용
button = driver.find_element(By.CSS_SELECTOR, '#login-btn')
button.click()

# 데이터 추출
content = driver.page_source
```

### 3.2 Playwright
Microsoft에서 개발한 최신 자동화 도구입니다. Selenium보다 빠르고, 비동기 처리를 지원하며, 자동 대기(Auto-wait) 기능이 강력합니다.

```python
from playwright.sync_api import sync_playwright

def run():
    with sync_playwright() as p:
        # 브라우저 실행 (chromium, firefox, webkit 지원)
        browser = p.chromium.launch(headless=False)
        page = browser.new_page()
        
        page.goto('https://www.example.com')
        
        # 클릭 및 입력
        page.click('text=Login')
        page.fill('input[name="user"]', 'myuser')
        
        # 데이터 추출
        title = page.title()
        print(title)
        
        browser.close()

if __name__ == '__main__':
    run()
```

> **주의 (Jupyter Notebook)**: Jupyter Notebook은 이미 비동기 루프(asyncio loop)가 실행 중이므로 `sync_playwright`를 사용하면 에러가 발생합니다. 대신 `async_playwright`를 사용해야 합니다.

```python
# Jupyter Notebook용 코드
import asyncio
from playwright.async_api import async_playwright

async def main():
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False)
        page = await browser.new_page()
        await page.goto('https://www.example.com')
        print(await page.title())
        await browser.close()

# Jupyter에서는 이미 루프가 돌고 있으므로 await main()으로 실행
await main()
```
### 3.3 동적 크롤링 문법 요약

Selenium과 Playwright의 주요 기능을 비교 정리했습니다.

| 기능 | Selenium (`driver.`) | Playwright (`page.`) | 설명 |
| :--- | :--- | :--- | :--- |
| **이동** | `get(url)` | `goto(url)` | 페이지 이동 |
| **찾기** | `find_element(By.CSS_SELECTOR, '...')` | `locator('...')` | 요소 찾기 |
| **클릭** | `element.click()` | `click('selector')` | 요소 클릭 |
| **입력** | `element.send_keys('text')` | `fill('selector', 'text')` | 텍스트 입력 |
| **대기** | `WebDriverWait` (명시적 대기) | 자동 대기 (Auto-wait) | 요소가 준비될 때까지 대기 |
| **텍스트** | `element.text` | `locator.inner_text()` | 텍스트 가져오기 |
| **스크린샷**| `save_screenshot('a.png')` | `screenshot(path='a.png')` | 화면 캡처 |

#### Selenium 예제 패턴
```python
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# 요소가 나타날 때까지 최대 10초 대기
element = WebDriverWait(driver, 10).until(
    EC.presence_of_element_located((By.CSS_SELECTOR, ".my-element"))
)
element.click()
```

#### Playwright 예제 패턴
```python
# Playwright는 기본적으로 요소가 상호작용 가능할 때까지 기다립니다.
page.click('text=로그인')

# 특정 상태 대기 (예: 네트워크 유휴 상태)
page.wait_for_load_state('networkidle')

# 요소가 화면에 보일 때까지 대기
page.wait_for_selector('.content', state='visible')
```
*   **Codegen**: `playwright codegen wikipedia.org` 명령어를 터미널에 입력하면, 브라우저에서의 동작을 자동으로 코드로 만들어줍니다.

---

## 4. 크롤링 프레임워크: Scrapy

대규모 데이터를 빠르고 효율적으로 수집해야 할 때 사용하는 프레임워크입니다. 비동기 네트워크 라이브러리인 Twisted를 기반으로 합니다.

### 4.1 특징
*   **비동기 처리**: 여러 페이지를 동시에 빠르게 크롤링.
*   **파이프라인**: 데이터 수집 -> 가공 -> 저장(DB/파일)의 구조화된 흐름 제공.
*   **Middlewares**: 요청/응답 처리 과정을 커스터마이징 가능.

### 4.2 기본 구조 (Spider 예시)
```python
import scrapy

class MySpider(scrapy.Spider):
    name = 'myspider'
    start_urls = ['https://www.example.com']

    def parse(self, response):
        # 데이터 추출
        for item in response.css('div.product'):
            yield {
                'title': item.css('h2::text').get(),
                'price': item.css('span.price::text').get(),
            }
        
        # 다음 페이지 링크 따라가기 (Pagination)
        next_page = response.css('a.next::attr(href)').get()
        if next_page is not None:
            yield response.follow(next_page, self.parse)
```

---

## 5. 심화 주제

### 5.1 에러 핸들링 (Error Handling)
크롤링 중에는 네트워크 오류, 타임아웃 등 다양한 예외가 발생할 수 있습니다. 안정적인 크롤러를 만들기 위해 **[08. python_exception_handling.md](08.%20python_exception_handling.md)**를 참고하여 견고한 예외 처리 로직을 작성하세요.

### 5.2 프로젝트 구조화
크롤링 코드가 복잡해지면 파일과 디렉토리를 체계적으로 관리해야 합니다. **[14. python_project_structure.md](14.%20python_project_structure.md)**를 참고하여 유지보수하기 좋은 프로젝트 구조를 설계해 보세요.

---

## 6. 도구 비교 및 요약

| 도구 | 유형 | 장점 | 단점 | 추천 상황 |
| :--- | :--- | :--- | :--- | :--- |
| **Requests + BS4** | 정적 | 매우 빠름, 가벼움, 쉬움 | JS 렌더링 불가, 상호작용 불가 | 단순 HTML 페이지, API 호출 |
| **Selenium** | 동적 | 거대한 커뮤니티, 다양한 예제 | 느림, 설정이 번거로울 수 있음 | 레거시 프로젝트 유지보수 |
| **Playwright** | 동적 | **매우 빠름**, 최신 기능, 자동 대기 | 비교적 최신이라 자료가 적음 | **동적 페이지 크롤링의 새로운 표준** |
| **Scrapy** | 프레임워크 | **압도적인 속도**, 구조적 관리 | 학습 곡선이 높음 | 쇼핑몰 전체 상품 등 **대규모 수집** |

> **Tip**: 처음에는 `Requests + BS4`로 시도하고, 동적 기능이 필요하면 `Playwright`를 사용하는 것을 추천합니다. 수집 규모가 매우 커지면 `Scrapy` 도입을 고려하세요.
